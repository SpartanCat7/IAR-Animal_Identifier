{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_FILES_DIRECTORY = \"./TEMP/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(TEMP_FILES_DIRECTORY + 'training_X.npy')\n",
    "Y = np.load(TEMP_FILES_DIRECTORY + 'training_Y.npy')\n",
    "\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[[0.39215686]\n  [0.37647059]\n  [0.37254902]\n  ...\n  [0.3372549 ]\n  [0.3372549 ]\n  [0.34509804]]\n\n [[0.39215686]\n  [0.32156863]\n  [0.31372549]\n  ...\n  [0.30196078]\n  [0.30196078]\n  [0.36078431]]\n\n [[0.37647059]\n  [0.34117647]\n  [0.30196078]\n  ...\n  [0.31764706]\n  [0.31764706]\n  [0.36470588]]\n\n ...\n\n [[0.36862745]\n  [0.35294118]\n  [0.46666667]\n  ...\n  [0.35686275]\n  [0.31764706]\n  [0.27058824]]\n\n [[0.50196078]\n  [0.48235294]\n  [0.63137255]\n  ...\n  [0.39607843]\n  [0.40784314]\n  [0.39607843]]\n\n [[0.48627451]\n  [0.53333333]\n  [0.61960784]\n  ...\n  [0.45098039]\n  [0.45882353]\n  [0.36470588]]]\n0\n2\n3\n3\n2\n"
    }
   ],
   "source": [
    "print(X[0])\n",
    "for i in Y[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CATEGORIES = [\"CAT\", \"CHICKEN\", \"DOG\", \"HORSE\"]\n",
    "\n",
    "CATEGORIES_TO_TRAIN = [\"CAT\", \"CHICKEN\", \"DOG\", \"HORSE\"]\n",
    "\n",
    "training_sets = []\n",
    "\n",
    "for category in CATEGORIES_TO_TRAIN:\n",
    "    this_category_Y = []\n",
    "    this_category_id = CATEGORIES.index(category)\n",
    "\n",
    "    for category_id in Y:\n",
    "        \n",
    "        if category_id == this_category_id:\n",
    "            this_category_Y.append(1)\n",
    "        else:\n",
    "            this_category_Y.append(0)\n",
    "    \n",
    "    this_category_Y = np.asarray(this_category_Y)\n",
    "\n",
    "    new_training_set = [category, 0, this_category_Y]   # 0 is a placeholder to replace with the model in the next step\n",
    "    training_sets.append(new_training_set)\n",
    "    #print(category)\n",
    "    #print(this_category_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CAT\n1 - 0 - 0 - 0 - 0 - \nCHICKEN\n0 - 0 - 0 - 0 - 0 - \nDOG\n0 - 1 - 0 - 0 - 1 - \nHORSE\n0 - 0 - 1 - 1 - 0 - \n"
    }
   ],
   "source": [
    "for category, model, category_Y in training_sets:\n",
    "    print(category)\n",
    "    for y in category_Y[:5]:\n",
    "        print(y, end=' - ')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for item in training_sets:\n",
    "    new_model = Sequential()\n",
    "    new_model.add(Conv2D(64,(3,3),input_shape = X.shape[1:]))\n",
    "    new_model.add(Activation(\"relu\"))\n",
    "    new_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    new_model.add(Conv2D(64,(3,3)))\n",
    "    new_model.add(Activation(\"relu\"))\n",
    "    new_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    new_model.add(Flatten())\n",
    "    new_model.add(Dense(64))\n",
    "\n",
    "    new_model.add(Dense(1))\n",
    "    new_model.add(Activation('sigmoid'))\n",
    "\n",
    "    training_sets[training_sets.index(item)][1] = new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/2\n17/17 [==============================] - 11s 622ms/step - loss: 0.7754 - accuracy: 0.7019 - val_loss: 0.5413 - val_accuracy: 0.7500\nEpoch 2/2\n17/17 [==============================] - 10s 609ms/step - loss: 0.5072 - accuracy: 0.7685 - val_loss: 0.4570 - val_accuracy: 0.7833\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2810448a2b0>"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "test_model = Sequential()\n",
    "\n",
    "test_model.add(Conv2D(64,(3,3),input_shape = X.shape[1:]))\n",
    "test_model.add(Activation(\"relu\"))\n",
    "test_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "test_model.add(Conv2D(64,(3,3)))\n",
    "test_model.add(Activation(\"relu\"))\n",
    "test_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "test_model.add(Flatten())\n",
    "test_model.add(Dense(64))\n",
    "\n",
    "test_model.add(Dense(1))\n",
    "test_model.add(Activation('sigmoid'))\n",
    "\n",
    "test_model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n",
    "test_model.fit(X, training_sets[3][2], batch_size=32, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.18431395]\n [0.25649616]\n [0.13681895]\n [0.13083059]\n [0.04829708]\n [0.43616343]\n [0.10443076]\n [0.09022343]\n [0.18209869]\n [0.09013161]\n [0.17039931]\n [0.26817292]\n [0.36091396]\n [0.19325495]\n [0.12913927]\n [0.0847722 ]\n [0.25807297]\n [0.43489453]\n [0.08806261]\n [0.12422848]\n [0.14921841]\n [0.16798973]\n [0.06835511]\n [0.13164946]\n [0.27418765]\n [0.37421107]\n [0.09760717]\n [0.6194181 ]\n [0.15614435]\n [0.3165917 ]\n [0.18768784]\n [0.27590168]\n [0.1758925 ]\n [0.10781926]\n [0.10110003]\n [0.11104512]\n [0.32784194]\n [0.1631448 ]\n [0.23093116]\n [0.23570207]\n [0.06079316]\n [0.12643841]\n [0.05116579]\n [0.70902455]\n [0.10794216]\n [0.2686702 ]\n [0.5515251 ]\n [0.15328717]\n [0.15470138]\n [0.104009  ]\n [0.11257705]\n [0.3191212 ]\n [0.10397226]\n [0.1073412 ]\n [0.06980893]\n [0.04297188]\n [0.21656111]\n [0.09400955]\n [0.23342302]\n [0.17342967]\n [0.12291232]\n [0.08095434]\n [0.22561815]\n [0.05814585]\n [0.10873893]\n [0.17209527]\n [0.11801839]\n [0.02637202]\n [0.20336723]\n [0.04320085]\n [0.09394044]\n [0.09987032]\n [0.22439885]\n [0.16596103]\n [0.11330467]\n [0.14958179]\n [0.21021   ]\n [0.17067075]\n [0.1300661 ]\n [0.07239228]\n [0.14950898]\n [0.14691633]\n [0.1563183 ]\n [0.4557606 ]\n [0.0690552 ]\n [0.091093  ]\n [0.12871313]\n [0.11451831]\n [0.05111286]\n [0.07077456]\n [0.13188764]\n [0.15233502]\n [0.12497547]\n [0.2666142 ]\n [0.4674899 ]\n [0.14787921]\n [0.17500287]\n [0.16063428]\n [0.08366185]\n [0.17311537]\n [0.06091169]\n [0.25314313]\n [0.06469154]\n [0.0994738 ]\n [0.89014447]\n [0.593139  ]\n [0.33568788]\n [0.33177775]\n [0.1770899 ]\n [0.26958373]\n [0.05009723]\n [0.3183009 ]\n [0.09688163]\n [0.12463972]\n [0.03573367]\n [0.06310704]\n [0.0435259 ]\n [0.16916245]\n [0.201291  ]\n [0.1930762 ]\n [0.21627253]\n [0.07871947]\n [0.064937  ]\n [0.18869984]\n [0.07962513]\n [0.06553882]\n [0.06084809]\n [0.08184713]\n [0.3526991 ]\n [0.10099816]\n [0.14149633]\n [0.04726261]\n [0.09784386]\n [0.0623458 ]\n [0.37806362]\n [0.1889557 ]\n [0.0771744 ]\n [0.07725412]\n [0.10861415]\n [0.14486116]\n [0.09874228]\n [0.11194888]\n [0.04668957]\n [0.10579044]\n [0.06817344]\n [0.08289137]\n [0.10087535]\n [0.07257476]\n [0.13473159]\n [0.05039498]\n [0.12907729]\n [0.30567652]\n [0.25916225]\n [0.16173473]\n [0.12845057]\n [0.13927549]\n [0.113646  ]\n [0.27278918]\n [0.07847565]\n [0.07497308]\n [0.10206982]\n [0.0893676 ]\n [0.81161475]\n [0.32841647]\n [0.15359941]\n [0.36038607]\n [0.23623002]\n [0.16360903]\n [0.16299045]\n [0.06333041]\n [0.09114802]\n [0.13494843]\n [0.10644469]\n [0.34498817]\n [0.28148445]\n [0.17635891]\n [0.08068064]\n [0.1954427 ]\n [0.14079374]\n [0.13299567]\n [0.15620476]\n [0.09843606]\n [0.13103613]\n [0.09446514]\n [0.13337421]\n [0.2561925 ]\n [0.15507928]\n [0.13087416]\n [0.19641894]\n [0.09312388]\n [0.09676769]\n [0.03936568]\n [0.17311332]\n [0.08839524]\n [0.10555604]\n [0.11707157]\n [0.7077221 ]\n [0.11582121]\n [0.2003493 ]\n [0.12948555]\n [0.05875   ]\n [0.1743576 ]\n [0.1461606 ]\n [0.06734774]\n [0.36525428]\n [0.14983961]\n [0.46189493]\n [0.17118093]\n [0.13773465]\n [0.09319001]\n [0.06253433]\n [0.44144225]\n [0.06406206]\n [0.08415756]\n [0.14974675]\n [0.1612443 ]\n [0.08955345]\n [0.49326783]\n [0.13512596]\n [0.07288036]\n [0.20936766]\n [0.14071888]\n [0.18584538]\n [0.11117038]\n [0.13383183]\n [0.0141989 ]\n [0.05378386]\n [0.28257602]\n [0.596701  ]\n [0.05110624]\n [0.18101269]\n [0.15973717]\n [0.1195083 ]\n [0.16446659]\n [0.12581068]\n [0.22371581]\n [0.09912288]\n [0.12264043]\n [0.09596652]\n [0.18803436]\n [0.18151608]\n [0.08511853]\n [0.48085606]\n [0.18253347]\n [0.12320656]\n [0.17232847]\n [0.1078805 ]\n [0.2502643 ]\n [0.21911258]\n [0.17241949]\n [0.32327443]\n [0.16057053]\n [0.1088995 ]\n [0.1047909 ]\n [0.04653364]\n [0.08341581]\n [0.13460839]\n [0.09390467]\n [0.05601901]\n [0.19482118]\n [0.27327734]\n [0.4559897 ]\n [0.14550167]\n [0.09937045]\n [0.32050365]\n [0.16254863]\n [0.03761098]\n [0.09720242]\n [0.10472533]\n [0.20071062]\n [0.27966544]\n [0.12283814]\n [0.09575224]\n [0.17034373]\n [0.15797147]\n [0.2410695 ]\n [0.13234457]\n [0.04020023]\n [0.44355327]\n [0.21960661]\n [0.23001668]\n [0.09945321]\n [0.06383494]\n [0.17901322]\n [0.16787702]\n [0.16128984]\n [0.14158025]\n [0.12805045]\n [0.2262595 ]\n [0.35410893]\n [0.11309215]\n [0.13011327]\n [0.22068727]\n [0.3725764 ]\n [0.22137457]\n [0.1933468 ]\n [0.31029493]\n [0.10203683]\n [0.37478238]\n [0.06732899]\n [0.31900686]\n [0.3664632 ]\n [0.26992512]\n [0.07442626]\n [0.12065601]\n [0.85999095]\n [0.06395936]\n [0.10653606]\n [0.15317485]\n [0.04069316]\n [0.1890848 ]\n [0.2221565 ]\n [0.06202796]\n [0.26338252]\n [0.13320008]\n [0.12941268]\n [0.14561135]\n [0.06795037]\n [0.44149405]\n [0.33170068]\n [0.0751327 ]\n [0.2878641 ]\n [0.3939629 ]\n [0.1651533 ]\n [0.0796836 ]\n [0.07694668]\n [0.19502002]\n [0.08632547]\n [0.39182985]\n [0.34743154]\n [0.13184458]\n [0.20502359]\n [0.11242956]\n [0.09830439]\n [0.07585809]\n [0.04442629]\n [0.32412004]\n [0.19840032]\n [0.10206053]\n [0.09963015]\n [0.11976543]\n [0.16933617]\n [0.07826659]\n [0.41181812]\n [0.68027467]\n [0.12840214]\n [0.3094589 ]\n [0.09335965]\n [0.19810522]\n [0.18700317]\n [0.0774529 ]\n [0.09458315]\n [0.06846803]\n [0.18023625]\n [0.15772069]\n [0.13948414]\n [0.335014  ]\n [0.07012725]\n [0.33073378]\n [0.08391866]\n [0.40025264]\n [0.29192704]\n [0.29037285]\n [0.2706889 ]\n [0.05283192]\n [0.41144443]\n [0.15251502]\n [0.03127497]\n [0.8602227 ]\n [0.13997415]\n [0.43183902]\n [0.1937818 ]\n [0.09429839]\n [0.15544474]\n [0.10401121]\n [0.35308975]\n [0.25663692]\n [0.2515633 ]\n [0.2706442 ]\n [0.07254803]\n [0.24812743]\n [0.11562872]\n [0.2334204 ]\n [0.19249296]\n [0.12545764]\n [0.07529694]\n [0.11904594]\n [0.10272521]\n [0.17102972]\n [0.23469937]\n [0.12054959]\n [0.13542786]\n [0.38191205]\n [0.17499349]\n [0.0940266 ]\n [0.21925187]\n [0.17579186]\n [0.7490092 ]\n [0.25724578]\n [0.37516797]\n [0.17889038]\n [0.10564512]\n [0.11761674]\n [0.12907341]\n [0.26370454]\n [0.06735581]\n [0.44510415]\n [0.10467657]\n [0.20017317]\n [0.3351236 ]\n [0.12769729]\n [0.3073224 ]\n [0.08996272]\n [0.0384551 ]\n [0.11282337]\n [0.10923579]\n [0.17987326]\n [0.17450705]\n [0.17065743]\n [0.06644127]\n [0.22785014]\n [0.1562298 ]\n [0.0552085 ]\n [0.54026145]\n [0.60937107]\n [0.1569511 ]\n [0.10937017]\n [0.05963218]\n [0.23193836]\n [0.18224591]\n [0.04898775]\n [0.1320782 ]\n [0.13166466]\n [0.10523251]\n [0.05854684]\n [0.13764912]\n [0.309155  ]\n [0.05967227]\n [0.02024317]\n [0.1680677 ]\n [0.14270791]\n [0.09763983]\n [0.318143  ]\n [0.08239183]\n [0.20589137]\n [0.11482343]\n [0.15567982]\n [0.24420878]\n [0.10338694]\n [0.10624048]\n [0.1516099 ]\n [0.30052954]\n [0.61830676]\n [0.31088114]\n [0.3522747 ]\n [0.33355802]\n [0.15479323]\n [0.2580124 ]\n [0.22137219]\n [0.210031  ]\n [0.7063997 ]\n [0.456553  ]\n [0.23883459]\n [0.15644687]\n [0.11339024]\n [0.15904966]\n [0.05054721]\n [0.32744995]\n [0.18857849]\n [0.10516304]\n [0.08555025]\n [0.10807511]\n [0.09603107]\n [0.3245605 ]\n [0.1231831 ]\n [0.17761263]\n [0.11456832]\n [0.39343923]\n [0.08109063]\n [0.1384731 ]\n [0.20627221]\n [0.19326752]\n [0.20743448]\n [0.10885289]\n [0.37616926]\n [0.08872423]\n [0.3503342 ]\n [0.04701793]\n [0.18166324]\n [0.07368907]\n [0.10841554]\n [0.10786611]\n [0.3166145 ]\n [0.25854042]\n [0.35164815]\n [0.17921728]\n [0.3324117 ]\n [0.10445231]\n [0.11235315]\n [0.8810009 ]\n [0.17168143]\n [0.6001142 ]\n [0.277268  ]\n [0.09986243]\n [0.05726048]\n [0.09710369]\n [0.0786323 ]\n [0.15538907]\n [0.11341864]\n [0.21504647]\n [0.2776946 ]\n [0.1391556 ]\n [0.05313006]\n [0.19825095]\n [0.27311397]\n [0.12175417]\n [0.08849075]\n [0.13628569]\n [0.11809981]\n [0.20241073]\n [0.10745749]\n [0.15851238]\n [0.21921158]\n [0.19724551]\n [0.43566   ]\n [0.53058845]\n [0.07543272]\n [0.12291601]\n [0.2958032 ]\n [0.5569866 ]\n [0.13062915]\n [0.08654585]\n [0.40839413]\n [0.21561223]\n [0.11313429]\n [0.0728384 ]\n [0.09629688]\n [0.22958449]\n [0.13067085]\n [0.13250622]\n [0.05075812]\n [0.2329045 ]\n [0.29835665]\n [0.13906014]\n [0.6058725 ]\n [0.33571297]\n [0.18566754]\n [0.3335927 ]\n [0.10137528]\n [0.06055209]\n [0.33896282]\n [0.0908328 ]\n [0.32938886]\n [0.09868383]\n [0.08239231]\n [0.18670139]\n [0.16214132]\n [0.1910604 ]\n [0.1896964 ]\n [0.04378289]\n [0.21956515]\n [0.25510713]\n [0.07102302]\n [0.13904798]\n [0.18665856]\n [0.06114563]\n [0.39714378]\n [0.12932432]\n [0.3492866 ]\n [0.16784444]\n [0.08364293]\n [0.05826351]\n [0.34539592]\n [0.0261451 ]\n [0.16863659]\n [0.09884596]\n [0.14023158]\n [0.14441329]\n [0.25849527]\n [0.2834444 ]\n [0.16303852]\n [0.206727  ]\n [0.16927364]\n [0.18720564]\n [0.17133865]\n [0.1559661 ]\n [0.21222004]\n [0.2337507 ]\n [0.14044711]\n [0.3406976 ]\n [0.05250129]\n [0.14185789]\n [0.23171055]\n [0.12285241]\n [0.23275253]\n [0.31275576]\n [0.05089158]\n [0.12469494]\n [0.13224545]\n [0.44352195]]\n"
    }
   ],
   "source": [
    "test_prediction = test_model.predict(X)\n",
    "print(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in training_sets:\n",
    "    index = training_sets.index(item)\n",
    "    training_sets[index][1].compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/5\n9/9 [==============================] - 10s 1s/step - loss: 1.4886 - accuracy: 0.6185 - val_loss: 0.9736 - val_accuracy: 0.6667\nEpoch 2/5\n9/9 [==============================] - 9s 1s/step - loss: 0.5871 - accuracy: 0.7593 - val_loss: 0.6466 - val_accuracy: 0.6667\nEpoch 3/5\n9/9 [==============================] - 9s 1s/step - loss: 0.4988 - accuracy: 0.7593 - val_loss: 0.6058 - val_accuracy: 0.7000\nEpoch 4/5\n9/9 [==============================] - 9s 1s/step - loss: 0.4603 - accuracy: 0.7759 - val_loss: 0.5892 - val_accuracy: 0.7000\nEpoch 5/5\n9/9 [==============================] - 9s 1s/step - loss: 0.3998 - accuracy: 0.7981 - val_loss: 0.5717 - val_accuracy: 0.7167\nEpoch 1/5\n9/9 [==============================] - 10s 1s/step - loss: 1.0208 - accuracy: 0.5019 - val_loss: 0.6549 - val_accuracy: 0.8000\nEpoch 2/5\n9/9 [==============================] - 9s 1s/step - loss: 0.6106 - accuracy: 0.7444 - val_loss: 0.5278 - val_accuracy: 0.8000\nEpoch 3/5\n9/9 [==============================] - 9s 1s/step - loss: 0.5711 - accuracy: 0.7444 - val_loss: 0.4692 - val_accuracy: 0.8000\nEpoch 4/5\n9/9 [==============================] - 9s 1s/step - loss: 0.5366 - accuracy: 0.7444 - val_loss: 0.4620 - val_accuracy: 0.8000\nEpoch 5/5\n9/9 [==============================] - 9s 1s/step - loss: 0.5030 - accuracy: 0.7463 - val_loss: 0.4805 - val_accuracy: 0.8333\nEpoch 1/5\n9/9 [==============================] - 9s 1s/step - loss: 1.8574 - accuracy: 0.5407 - val_loss: 0.6102 - val_accuracy: 0.7667\nEpoch 2/5\n9/9 [==============================] - 10s 1s/step - loss: 0.5820 - accuracy: 0.7481 - val_loss: 0.5727 - val_accuracy: 0.7667\nEpoch 3/5\n9/9 [==============================] - 9s 1s/step - loss: 0.5788 - accuracy: 0.7481 - val_loss: 0.5431 - val_accuracy: 0.7667\nEpoch 4/5\n9/9 [==============================] - 9s 996ms/step - loss: 0.5711 - accuracy: 0.7481 - val_loss: 0.5516 - val_accuracy: 0.7667\nEpoch 5/5\n9/9 [==============================] - 9s 995ms/step - loss: 0.5534 - accuracy: 0.7481 - val_loss: 0.5556 - val_accuracy: 0.7667\nEpoch 1/5\n9/9 [==============================] - 9s 1s/step - loss: 0.9464 - accuracy: 0.6500 - val_loss: 0.5440 - val_accuracy: 0.7667\nEpoch 2/5\n9/9 [==============================] - 9s 993ms/step - loss: 0.5258 - accuracy: 0.7500 - val_loss: 0.4864 - val_accuracy: 0.7833\nEpoch 3/5\n9/9 [==============================] - 10s 1s/step - loss: 0.4574 - accuracy: 0.7889 - val_loss: 0.5318 - val_accuracy: 0.7167\nEpoch 4/5\n9/9 [==============================] - 10s 1s/step - loss: 0.4277 - accuracy: 0.8056 - val_loss: 0.4732 - val_accuracy: 0.7500\nEpoch 5/5\n9/9 [==============================] - 9s 1s/step - loss: 0.3806 - accuracy: 0.8500 - val_loss: 0.3861 - val_accuracy: 0.8333\n"
    }
   ],
   "source": [
    "for category, model, category_Y in training_sets:\n",
    "    index = training_sets.index([category, model, category_Y])\n",
    "\n",
    "    training_sets[index][1].fit(X, category_Y, batch_size=64, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Assets written to: SavedModels/CAT\\assets\nINFO:tensorflow:Assets written to: SavedModels/CHICKEN\\assets\nINFO:tensorflow:Assets written to: SavedModels/DOG\\assets\nINFO:tensorflow:Assets written to: SavedModels/HORSE\\assets\n"
    }
   ],
   "source": [
    "for category_name, model, category_Y in training_sets:\n",
    "    index = training_sets.index([category_name, model, category_Y])\n",
    "    training_sets[index][1].save('SavedModels/' + category_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[3.64028960e-01]\n [1.17211938e-02]\n [2.74106622e-01]\n [3.33832800e-01]\n [4.84816790e-01]\n [7.61848986e-02]\n [5.86750865e-01]\n [9.40451324e-02]\n [8.16018879e-02]\n [3.69525194e-01]\n [1.99511349e-01]\n [1.28242821e-01]\n [1.75810188e-01]\n [2.16409802e-01]\n [2.98196077e-02]\n [3.32702458e-01]\n [7.75085986e-02]\n [6.00920022e-02]\n [1.53148144e-01]\n [4.17631656e-01]\n [6.20419860e-01]\n [5.15952587e-01]\n [5.80893755e-01]\n [1.48001134e-01]\n [3.87677252e-02]\n [3.01864147e-02]\n [5.74907362e-02]\n [4.45085764e-03]\n [9.72986221e-03]\n [2.43117481e-01]\n [2.21359968e-01]\n [3.73750687e-01]\n [2.73034036e-01]\n [3.70004088e-01]\n [2.85605192e-01]\n [2.61934996e-02]\n [2.89501250e-02]\n [2.69199312e-01]\n [1.86339557e-01]\n [2.17498690e-01]\n [4.01159406e-01]\n [3.96128535e-01]\n [6.09783590e-01]\n [3.35529447e-03]\n [3.40023756e-01]\n [3.99934649e-02]\n [4.66122925e-02]\n [1.77200347e-01]\n [1.17158532e-01]\n [4.95399356e-01]\n [1.59181684e-01]\n [1.23839319e-01]\n [5.72639704e-02]\n [3.05029720e-01]\n [3.64613533e-02]\n [3.66403073e-01]\n [6.10059798e-02]\n [1.68180704e-01]\n [1.21377140e-01]\n [2.91156024e-01]\n [2.88168132e-01]\n [6.07672632e-01]\n [4.10784870e-01]\n [5.53397417e-01]\n [9.69195068e-02]\n [4.31559980e-02]\n [2.09707290e-01]\n [5.63667655e-01]\n [5.90838253e-01]\n [5.81233978e-01]\n [4.66079444e-01]\n [2.52329022e-01]\n [1.72618717e-01]\n [1.09069347e-01]\n [4.55584705e-01]\n [1.06784642e-01]\n [3.07881296e-01]\n [1.92072690e-02]\n [9.89028215e-02]\n [1.27754956e-01]\n [3.44840050e-01]\n [3.11088860e-01]\n [1.68918610e-01]\n [4.07743454e-03]\n [4.93283659e-01]\n [2.15482414e-02]\n [4.78351712e-01]\n [1.58622593e-01]\n [6.00594878e-02]\n [5.55838585e-01]\n [5.22621393e-01]\n [2.14221984e-01]\n [2.53699064e-01]\n [1.29363805e-01]\n [5.25091588e-02]\n [4.53732312e-02]\n [6.51792049e-01]\n [3.39584172e-01]\n [5.02505004e-01]\n [2.55374432e-01]\n [5.52253127e-01]\n [1.91728890e-01]\n [4.45080996e-01]\n [1.65146470e-01]\n [4.14487720e-03]\n [4.28323150e-02]\n [1.50991082e-02]\n [1.93859249e-01]\n [4.30093020e-01]\n [7.87011087e-02]\n [5.24666011e-01]\n [2.66271830e-02]\n [1.46927446e-01]\n [5.58801711e-01]\n [9.95082855e-02]\n [1.38171047e-01]\n [5.39531469e-01]\n [2.39548922e-01]\n [5.70883453e-01]\n [5.17077744e-02]\n [1.93626851e-01]\n [3.77077162e-01]\n [5.00582159e-01]\n [3.24308217e-01]\n [1.56574070e-01]\n [2.16700375e-01]\n [5.07178187e-01]\n [3.85300547e-01]\n [1.72494262e-01]\n [2.96659619e-01]\n [4.34022009e-01]\n [4.50893164e-01]\n [1.73055828e-02]\n [3.29382956e-01]\n [4.59659100e-03]\n [4.12730306e-01]\n [3.25947285e-01]\n [4.30524617e-01]\n [4.78815913e-01]\n [1.72374576e-01]\n [1.08380675e-01]\n [2.28786349e-01]\n [4.61236060e-01]\n [3.36496830e-01]\n [4.02049243e-01]\n [1.71812922e-01]\n [1.01557285e-01]\n [4.64690864e-01]\n [4.33491766e-02]\n [2.53710449e-02]\n [6.63631260e-02]\n [2.29131043e-01]\n [1.42533869e-01]\n [1.89123005e-01]\n [1.03992581e-01]\n [4.54184920e-01]\n [1.46663100e-01]\n [1.15167350e-01]\n [5.15567839e-01]\n [5.05763769e-01]\n [4.13442433e-01]\n [2.22608149e-02]\n [6.31749630e-04]\n [1.56162053e-01]\n [8.02792907e-02]\n [1.32823646e-01]\n [1.11946464e-02]\n [5.86754680e-02]\n [2.62284517e-01]\n [2.46585459e-01]\n [7.20252335e-01]\n [2.12058872e-01]\n [5.83095849e-01]\n [2.35140026e-02]\n [1.75477773e-01]\n [6.10620141e-01]\n [4.42494392e-01]\n [2.39915401e-01]\n [2.50377059e-02]\n [3.99526149e-01]\n [4.37604696e-01]\n [3.72203529e-01]\n [4.02294695e-01]\n [1.68983191e-01]\n [1.90218836e-01]\n [5.09041846e-02]\n [2.58639932e-01]\n [1.26085579e-02]\n [1.76507860e-01]\n [3.56273174e-01]\n [1.04232997e-01]\n [7.18270063e-01]\n [8.93442631e-02]\n [4.44836587e-01]\n [4.91481632e-01]\n [6.57215118e-02]\n [2.14653015e-02]\n [2.31422931e-01]\n [2.38738060e-02]\n [2.02542543e-02]\n [4.60613400e-01]\n [3.00680518e-01]\n [4.51693892e-01]\n [2.40371138e-01]\n [4.72925305e-02]\n [4.12797540e-01]\n [1.06280506e-01]\n [3.49145174e-01]\n [1.86803162e-01]\n [2.55194962e-01]\n [7.62913525e-02]\n [3.28365266e-02]\n [4.73163784e-01]\n [3.19351673e-01]\n [2.83124983e-01]\n [5.97003698e-02]\n [1.86848909e-01]\n [2.03676820e-02]\n [4.60647643e-01]\n [8.90784860e-01]\n [1.51608557e-01]\n [1.22304350e-01]\n [8.13018084e-02]\n [9.72601771e-02]\n [3.15183908e-01]\n [8.88043880e-01]\n [7.89553285e-01]\n [2.21401334e-01]\n [6.07383251e-03]\n [3.29096258e-01]\n [4.30093110e-01]\n [5.14744282e-01]\n [2.82366216e-01]\n [1.20398611e-01]\n [1.33413255e-01]\n [3.81564796e-02]\n [4.65852857e-01]\n [1.12017989e-01]\n [3.24038088e-01]\n [1.07158959e-01]\n [1.61790997e-01]\n [5.55370927e-01]\n [3.52144241e-04]\n [1.90731704e-01]\n [5.76054454e-02]\n [2.03044981e-01]\n [2.40342945e-01]\n [3.34971964e-01]\n [2.45743096e-02]\n [2.71365345e-01]\n [7.95101225e-02]\n [3.20549697e-01]\n [1.47051603e-01]\n [3.28495979e-01]\n [1.23446524e-01]\n [5.03647149e-01]\n [4.85062152e-01]\n [6.79003298e-02]\n [4.95631814e-01]\n [2.57767856e-01]\n [1.63468033e-01]\n [8.27348232e-03]\n [1.74518585e-01]\n [5.34286261e-01]\n [1.37930334e-01]\n [2.71953702e-01]\n [4.33965504e-01]\n [1.04155451e-01]\n [5.76743901e-01]\n [1.51820004e-01]\n [2.24864393e-01]\n [3.33956331e-01]\n [4.99003530e-01]\n [1.89182639e-01]\n [3.49369526e-01]\n [5.29133379e-02]\n [3.33812594e-01]\n [4.11464870e-01]\n [7.61464238e-03]\n [8.47275555e-02]\n [1.08606189e-01]\n [6.23899877e-01]\n [5.89825749e-01]\n [3.62176120e-01]\n [1.58011168e-01]\n [4.74812388e-02]\n [2.50557274e-01]\n [2.81966001e-01]\n [2.85354853e-02]\n [2.28611231e-02]\n [4.53729123e-01]\n [2.16575980e-01]\n [5.38108051e-02]\n [7.93045759e-03]\n [7.11880624e-02]\n [1.50257826e-01]\n [7.60436952e-02]\n [5.05907297e-01]\n [9.97596681e-02]\n [3.39795589e-01]\n [1.24797702e-01]\n [1.84923410e-03]\n [7.36813247e-02]\n [6.32067680e-01]\n [4.40329790e-01]\n [6.11591339e-03]\n [4.81819153e-01]\n [3.90575230e-02]\n [3.14763367e-01]\n [1.75206661e-02]\n [7.98093677e-02]\n [9.50800776e-02]\n [5.18694758e-01]\n [9.07400250e-02]\n [4.48423415e-01]\n [3.00621152e-01]\n [1.33636713e-01]\n [5.10563672e-01]\n [1.63004398e-02]\n [2.91450620e-02]\n [4.25496221e-01]\n [1.74447984e-01]\n [4.37683165e-02]\n [4.15066481e-01]\n [3.09060365e-01]\n [3.73458207e-01]\n [3.70743871e-02]\n [4.18928087e-01]\n [5.81987798e-02]\n [2.44115889e-02]\n [3.71255696e-01]\n [9.69844460e-02]\n [5.54493487e-01]\n [3.88085186e-01]\n [1.90760583e-01]\n [5.38163006e-01]\n [5.08562326e-02]\n [4.90912229e-01]\n [2.31444895e-01]\n [2.40089953e-01]\n [4.30956334e-01]\n [1.28282309e-02]\n [4.07210976e-01]\n [4.01991010e-02]\n [1.27257705e-02]\n [1.20709836e-01]\n [5.99504113e-02]\n [4.61200058e-01]\n [4.26605731e-01]\n [9.43474174e-02]\n [5.67529380e-01]\n [4.65149462e-01]\n [1.65488929e-01]\n [3.68132353e-01]\n [3.17167521e-01]\n [2.52788097e-01]\n [6.79678917e-02]\n [5.09570062e-01]\n [6.03699684e-03]\n [2.50410676e-01]\n [3.41319442e-02]\n [1.49930686e-01]\n [7.16741383e-02]\n [2.81986296e-02]\n [5.33966660e-01]\n [2.47384012e-02]\n [3.81294936e-01]\n [7.35315502e-01]\n [9.86099243e-04]\n [1.67743087e-01]\n [5.92941940e-02]\n [3.56266230e-01]\n [4.78603452e-01]\n [6.19418740e-01]\n [3.71953785e-01]\n [1.15063518e-01]\n [2.51035810e-01]\n [1.22288197e-01]\n [6.21542633e-02]\n [5.16842604e-02]\n [9.61747766e-03]\n [6.62799478e-02]\n [3.23284566e-01]\n [2.59144723e-01]\n [4.45103109e-01]\n [4.34281111e-01]\n [1.91296130e-01]\n [2.52507865e-01]\n [1.88219905e-01]\n [3.74969840e-03]\n [4.26579446e-01]\n [3.47676933e-01]\n [9.45642591e-03]\n [4.49300587e-01]\n [3.58214140e-01]\n [1.76436067e-01]\n [2.78094888e-01]\n [5.02169132e-03]\n [1.22115195e-01]\n [3.04882824e-02]\n [4.52172190e-01]\n [6.07238352e-01]\n [2.81726182e-01]\n [3.24806601e-01]\n [6.25196993e-02]\n [3.80972385e-01]\n [1.01673484e-01]\n [9.19679999e-02]\n [3.09486806e-01]\n [2.09535658e-02]\n [5.09097397e-01]\n [1.48098201e-01]\n [7.64696121e-01]\n [4.01456237e-01]\n [3.48569125e-01]\n [5.84476531e-01]\n [4.41934437e-01]\n [3.50369871e-01]\n [3.62304270e-01]\n [4.83879119e-01]\n [1.70262456e-01]\n [2.33471632e-01]\n [5.08693755e-02]\n [2.59741843e-02]\n [5.46467304e-03]\n [3.82059038e-01]\n [4.77811396e-02]\n [4.71405566e-01]\n [9.04618502e-02]\n [5.75682104e-01]\n [3.90738249e-03]\n [2.19860822e-01]\n [4.54519093e-01]\n [2.07456857e-01]\n [5.30937314e-01]\n [3.69154990e-01]\n [1.38407946e-02]\n [5.44006586e-01]\n [8.42321277e-01]\n [3.57656598e-01]\n [1.79901123e-01]\n [3.48791212e-01]\n [8.48366320e-02]\n [4.37110543e-01]\n [3.46029878e-01]\n [2.36995220e-01]\n [1.68600231e-01]\n [1.25757843e-01]\n [5.90707839e-01]\n [2.58485615e-01]\n [3.85125399e-01]\n [1.97731644e-01]\n [7.49771297e-02]\n [2.32146502e-01]\n [3.77486944e-02]\n [9.14135575e-03]\n [1.96757257e-01]\n [3.68955135e-02]\n [1.81736022e-01]\n [1.54946953e-01]\n [4.29334939e-02]\n [2.87809670e-02]\n [1.04622155e-01]\n [3.82818699e-01]\n [3.15521300e-01]\n [3.58894467e-03]\n [6.03305042e-01]\n [4.56323922e-02]\n [2.54364163e-01]\n [5.49239337e-01]\n [4.07183558e-01]\n [6.06109500e-02]\n [3.52483898e-01]\n [4.26068306e-02]\n [1.73742026e-01]\n [2.05941945e-01]\n [1.93272620e-01]\n [3.34420800e-02]\n [3.83523822e-01]\n [2.25367844e-01]\n [2.37211317e-01]\n [3.83689493e-01]\n [9.71190631e-02]\n [5.28921068e-01]\n [1.66142881e-02]\n [1.37073696e-01]\n [9.43540037e-02]\n [3.23564410e-01]\n [5.55096030e-01]\n [4.31048065e-01]\n [4.16392654e-01]\n [3.69245797e-01]\n [3.36979985e-01]\n [2.45910317e-01]\n [7.60704279e-03]\n [3.05425823e-01]\n [3.60669464e-01]\n [2.23777682e-01]\n [3.57191682e-01]\n [1.63674355e-03]\n [2.25219250e-01]\n [7.73411989e-03]\n [1.81112885e-02]\n [3.31482112e-01]\n [5.14823318e-01]\n [2.52232552e-02]\n [4.33461010e-01]\n [3.47252011e-01]\n [4.15261209e-01]\n [2.15884924e-01]\n [1.04710728e-01]\n [1.60810411e-01]\n [5.42415977e-01]\n [1.09822839e-01]\n [1.96355462e-01]\n [5.60828149e-01]\n [2.61989087e-01]\n [2.38348871e-01]\n [4.10166234e-01]\n [3.17396969e-01]\n [4.35931653e-01]\n [1.94713771e-01]\n [5.76449931e-02]\n [3.04813385e-01]\n [5.71528971e-02]\n [3.18385661e-02]\n [8.29888880e-02]\n [5.50254703e-01]\n [3.53654772e-01]\n [3.48452330e-02]\n [2.56094038e-01]\n [1.53196543e-01]\n [4.83258069e-02]\n [2.53078997e-01]\n [4.18913245e-01]\n [2.29599535e-01]\n [3.83324713e-01]\n [3.12449038e-01]\n [2.69428790e-02]\n [2.31121689e-01]\n [4.84272689e-01]\n [3.85963917e-02]\n [1.82826698e-01]\n [4.22181845e-01]\n [1.73089802e-02]\n [1.35254651e-01]\n [1.72780544e-01]\n [1.42368823e-01]\n [4.44633245e-01]\n [3.60300243e-01]\n [4.41843271e-02]\n [3.52756292e-01]\n [1.37675196e-01]\n [3.96650702e-01]\n [4.54696178e-01]\n [1.48521543e-01]\n [2.31777191e-01]\n [8.74717236e-02]\n [6.42150342e-02]\n [3.11999232e-01]\n [2.68599510e-01]\n [2.82335371e-01]\n [2.52586395e-01]\n [2.71513551e-01]\n [1.03933871e-01]\n [3.95446420e-02]\n [2.02387899e-01]\n [3.02736253e-01]\n [8.03507864e-02]\n [3.82063925e-01]\n [3.36338758e-01]\n [2.78216153e-01]\n [7.22619891e-02]\n [5.97337842e-01]\n [1.45917505e-01]\n [2.34524608e-02]\n [3.64149988e-01]\n [5.18978477e-01]\n [2.27221817e-01]\n [1.27592862e-01]\n [4.59528983e-01]\n [2.47830033e-01]\n [3.03787589e-02]\n [1.04217261e-01]\n [5.86821437e-01]\n [8.45651329e-02]\n [1.93511486e-01]\n [7.08150864e-03]\n [1.83603525e-01]\n [8.16221237e-02]\n [5.55246949e-01]\n [1.80364221e-01]\n [3.51261258e-01]\n [4.88281459e-01]\n [1.07161909e-01]\n [1.49629503e-01]\n [5.09790361e-01]\n [1.17294073e-01]\n [3.62631887e-01]\n [4.82726395e-02]]\n"
    }
   ],
   "source": [
    "test_prediction = training_sets[0][1].predict(X)\n",
    "print(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}